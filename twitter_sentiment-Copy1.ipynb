{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Twitter Moral Corpus} $$\n",
    "$$ \\text{and} $$\n",
    "$$ \\text{H2O's Python Client with Driverless AI} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes H2O's Driverless AI and related Python Client to analyze the Twitter Corpus Paper 'Moral Foundations Twitter Corpus: A collection of 35k tweets annotated for moralsentiment.' The goal is to analyze the Twitter Moral Corpus and test the efficacy and usability of H2O's automated machine learning through their H2O Ai Client.  \n",
    "\n",
    "Moral Foundations Theory is a five factor taxonomy of human morality. Below are the five.\n",
    "\n",
    "Moral Factors:\n",
    "- Care\\Harm\n",
    "- Fairness\\Cheating\n",
    "- Loyalty\\Betrayal\n",
    "- Authority\\Subversion\n",
    "- Purity\\Degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget #!pip install wget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from h2oai_client import Client\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download twitter data with tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tw.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://drive.google.com/u/0/uc?id=1d1p95CspLTT1em4I42rDpWY6Hcikg296&export=download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter data doesn't come with tweets. In order to get the tweets you have to sign up for a twitter develop account. \n",
    "\n",
    "You will need to collect the following and fill in the values in the text_script before you run it with python text_script.py. The tweets will then be populated. <br>\n",
    "Values to be filled: consumer_key='', consumer_secret='', access_token_key='', access_token_secret=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter sentiment data\n",
    "# wget.download('https://osf.io/cwu4m/download')\n",
    "# os.rename('MFTC_V4.json', 'twitter_sentiment.json')\n",
    "\n",
    "# Text script\n",
    "# wget.download('https://osf.io/mzg5w/download')\n",
    "# !python text_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The davidson comes separate. Below will download that; however, this was left out because the word classications weren't available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wget for Davidson dataset mentioned below\n",
    "# wget.download('https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/ \\\n",
    "# master/data/labeled_data.csv')\n",
    "# df_davidson = pd.read_csv('labeled_data.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set credentials for H2O Client. The username will be the beginning of the email used for your GCP account. You can also type ‘id’ into the terminal and that will show the username, i.e. uid=1001(username). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'http://33.106.36.34:12345'\n",
    "username = 'scidataprojects'\n",
    "password = 'nnnnn'\n",
    "h2oai = Client(address = address, username = username, password = password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import moral corpus', below is each given corpus. The Davidson corpus needed to be downloaded from elsewhere, and didn't have the actual classification words included so was ultimately excluded from the analysis.  \n",
    "\n",
    "Domains are selected based on relevant moral problems in social sciences.\n",
    "- ALM: All Lives Matter - political right\n",
    "- Baltimore: Baltimore protests related to the death of Freddie Gray.\n",
    "- BLM: Black Lives Matter - political left\n",
    "- Election: The 2016 presidential election\n",
    "- MeToo: Womens sexual harassment/assualt movement. \n",
    "- Sandy: Hurricane Sandy\n",
    "- Davidson: Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('tw.json')\n",
    "dfs = []\n",
    "ALM, Baltimore, BLM, Election, MeToo, Sandy = df['Tweets'][0], df['Tweets'][1], df['Tweets'][2], \\\n",
    "                                              df['Tweets'][4], df['Tweets'][5], df['Tweets'][6]\n",
    "\n",
    "corpus = [ALM, Baltimore, BLM, Election, MeToo, Sandy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domains come embedded in a json file. For ease of use, the below code takes the json and places it in a pandas dataframe. All hash tags were removed from the tweets. Each tweet has the potential to have multiple word classifications based on the aforementioned moral theory factors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial lists to be converted to a dataframe later\n",
    "text_id = []\n",
    "tweet = []\n",
    "date = []\n",
    "annotation = []\n",
    "\n",
    "for c in corpus:\n",
    "    for i in c:\n",
    "        text_id.append(list(i.values())[0])\n",
    "        tweet.append(list(i.values())[1])\n",
    "        date.append(list(i.values())[2])\n",
    "        annotation.append(list(i.values())[3])\n",
    "\n",
    "tweet = pd.DataFrame(tweet, columns=['tweet'])\n",
    "text_id = pd.DataFrame(text_id, columns=['text_id'])\n",
    "date = pd.DataFrame(date, columns=['date'])\n",
    "\n",
    "frst_column_annotations = []\n",
    "scnd_column_annotations = []\n",
    "\n",
    "\n",
    "for i in annotation:\n",
    "    frst_column_annotations.append(list(i[0].values())[1])\n",
    "    scnd_column_annotations.append(list(i[1].values())[1])\n",
    "\n",
    "dataframe_a_words = pd.DataFrame(frst_column_annotations, columns=['word1'])\n",
    "dataframe_b_words = pd.DataFrame(scnd_column_annotations, columns=['word2'])\n",
    "words = pd.concat([dataframe_a_words, dataframe_b_words], axis=1)\n",
    "\n",
    "words_col_a = words['word1'].str.split(',', 3, expand=True).rename(columns={0:'w1', 1:'w2', 2:'w3', 3:'w4'})\n",
    "words_col_b = words['word2'].str.split(',', 3, expand=True).rename(columns={0:'w5', 1:'w6', 2:'w7', 3:'w8'})\n",
    "words_df = pd.concat([words_col_a, words_col_b], axis=1)\n",
    "\n",
    "# Rename columns\n",
    "text_id = pd.DataFrame(text_id, columns=['text_id'])\n",
    "tweet = pd.DataFrame(tweet, columns=['tweet'])\n",
    "date = pd.DataFrame(date, columns=['date'])\n",
    "\n",
    "df_list = [text_id, tweet, date, words_df]\n",
    "df = pd.concat(df_list, axis=1)\n",
    "\n",
    "df.drop(columns=['text_id', 'date'], inplace=True)\n",
    "df = df[df['tweet'] != 'no tweet text available']\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df['tweet'] = df['tweet'].str.replace('#\\S+', '', regex=True) \\\n",
    "                         .str.replace('&amp;','and') \\\n",
    "                         .str.strip()\n",
    "\n",
    "is_duplicate = df.apply(pd.Series.duplicated, axis=1)\n",
    "df_ = df.where(~is_duplicate, None)\n",
    "\n",
    "df = df_[['tweet','w1']].rename(columns={'w1':'Annotations'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a preview of the null counts from the separated words. We can see that the majority of additional labels tends towards missing a lot of values. For this reason only the first word is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>w7</th>\n",
       "      <th>w8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Null Counts:</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2107</td>\n",
       "      <td>2505</td>\n",
       "      <td>2602</td>\n",
       "      <td>922</td>\n",
       "      <td>2158</td>\n",
       "      <td>2520</td>\n",
       "      <td>2611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet  w1    w2    w3    w4   w5    w6    w7    w8\n",
       "Null Counts:      0   0  2107  2505  2602  922  2158  2520  2611"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_.isna().sum(), columns=['Null Counts:']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview of word '6' to show the proportion of nulls to words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35030395136778114"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['w5'].isna().sum() / len(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into test and train sets. We use numpy to split the data then export to csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = np.split(df.sample(frac=1), [int(.7*len(df))])\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "train.to_csv(\"train_twitter_sentiment.csv\", index=False)\n",
    "test.to_csv(\"test_twitter_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the data and place into the h2ai client to utilize their classes/tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train_twitter_sentiment.csv'\n",
    "test_path = './test_twitter_sentiment.csv'\n",
    "\n",
    "train = h2oai.upload_dataset_sync(train_path)\n",
    "test = h2oai.upload_dataset_sync(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the size of the dataset and the used columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:  2 x 1842\n",
      "Test Dataset:  2 x 790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tweet', 'Annotations']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train Dataset: ', len(train.columns), 'x', train.row_count)\n",
    "print('Test Dataset: ', len(test.columns), 'x', test.row_count)\n",
    "\n",
    "[c.name for c in train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually running a set of algos on your training set, the h2oai client has a function to preview a rough estimate of some of the parameters, i.e. how changing the accuracy and the time change the total time taken to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCURACY [10/10]:',\n",
       " '- Training data size: *1,842 rows, 2 cols*',\n",
       " '- Feature evolution: *[LightGBM, TensorFlow]*, *3-fold CV**, 3 reps*',\n",
       " '- Final pipeline: *Ensemble (12 models), 3-fold CV*',\n",
       " '',\n",
       " 'TIME [10/10]:',\n",
       " '- Feature evolution: *8 individuals*, up to *540 iterations*',\n",
       " '- Early stopping: After *50* iterations of no improvement',\n",
       " '',\n",
       " 'INTERPRETABILITY [10/10]:',\n",
       " '- Feature pre-pruning strategy: Permutation Importance FS',\n",
       " '- Monotonicity constraints: enabled',\n",
       " '- Feature engineering search space: [CVTargetEncode, Frequent, TextBiGRU, TextCNN, TextCharCNN, Text]',\n",
       " '',\n",
       " '[LightGBM, TensorFlow] models to train:',\n",
       " '- Model and feature tuning: *360*',\n",
       " '- Feature evolution: *18072*',\n",
       " '- Final pipeline: *12*',\n",
       " '',\n",
       " 'Estimated runtime: *hours*',\n",
       " 'Auto-click Finish/Abort if not done in: *1 day*/*7 days*']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_preview = h2oai.get_experiment_preview_sync(\n",
    "    dataset_key=train.key\n",
    "    , validset_key=''\n",
    "    , target_col='Annotations'\n",
    "    , classification=True\n",
    "    , dropped_cols=[]\n",
    "    , accuracy=10\n",
    "    , time=10\n",
    "    , interpretability=10\n",
    "    , is_time_series=False\n",
    "    , time_col=''\n",
    "    , enable_gpus=True\n",
    "    , reproducible=False\n",
    "    , resumed_experiment_id=''\n",
    "    , config_overrides=\"\"\"\n",
    "        enable_tensorflow='on'\n",
    "        enable_tensorflow_charcnn='on'\n",
    "        enable_tensorflow_textcnn='on'\n",
    "        enable_tensorflow_textbigru='on'\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "exp_preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the previewed model above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = h2oai.start_experiment_sync(\n",
    "    dataset_key=train.key,\n",
    "    testset_key=test.key,\n",
    "    target_col='Annotations',\n",
    "    scorer='Accuracy',\n",
    "    is_classification=True,\n",
    "    cols_to_drop=[],\n",
    "    accuracy=10,\n",
    "    time=10,\n",
    "    interpretability=10,\n",
    "    enable_gpus=True,\n",
    "    config_overrides=\"\"\"\n",
    "                    enable_tensorflow='on'\n",
    "                    enable_tensorflow_charcnn='on'\n",
    "                    enable_tensorflow_textcnn='on'\n",
    "                    enable_tensorflow_textbigru='on'\n",
    "                    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When complete, each model will have a unique key. This key may be used to run comparative analysis on different recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modeling completed for model ' + model.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the test predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = h2oai.download(model.test_predictions_path, '.')\n",
    "print('Test set predictions available at', test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('test_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
